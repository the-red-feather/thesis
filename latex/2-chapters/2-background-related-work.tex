%-------------------------------------------------------------------------------------------------%
% A related work section in which the relevant literature is presented and 
% linked to the project. 
% It should show that you clearly know the problem you plan to solve, 
% and that you master the related work. 

\chapter{Background \& Related work}
\label{chap:background}

\begin{figure}
  \centering
  \graphicspath{ {../../assets/diagrams/} }
  \includegraphics[width=270px]{geo-web-vpl.png}
  \caption{Triangle Model}
  \label{fig:triangle-model}
\end{figure}

This chapter offers an overview of the theoretical background that this study builds upon.
The study takes place at the intersection of three prior bodies of work,  represented by the corners of \reffig{fig:triangle-model}:
\begin{enumerate}[-]
  \item \refsec{sec:background-geo} covers the background on Geocomputation
  \item \refsec{sec:background-web} covers the background on Web applications
  \item \refsec{sec:background-vpl} covers the background on Visual Programming Languages
\end{enumerate}
Each one of these cornerstones will be discussed and analyzed \emph{an sich}, after which \refchap{chap:related} and \refchap{chap:methodology} will focus on the interplay and connections between these bodies of work. 

\section{Geocomputation}
\label{sec:background-geo}

This section offers a brief background on the wide topic of geocomputation. 

% PURPOSE: Show that you understand geo-computation | Show why the rest of the study will focus on the computer graphics side of things

Geocomputation is a central component of the wider field of geo-informatics. 
The term geocomputation, or geodata processing, is used to represent all types of computations performed on geographical data. 
Anything from the calculation of an area of a polygon, to \ac{crs} transformations, feature overlay, or converting a raster dataset into a vectorized dataset, is regarded as geocomputation.

It must be emphasized that a geocomputational procedure is always fully defined by and dependent upon its input and output data types (similar to any type of computation). 
This is also why geocomputation is seldom a \emph{goal} in itself, but much rather the \emph{means} to discovering geo-information. 

\begin{note}
  TODO: show images of geo-computation
\end{note}

\subsection{Similarities and differences with neighboring fields}

\begin{figure}
  \centering
  \graphicspath{ {../../assets/diagrams/} }
  \includegraphics[width=380px]{geocomputation.png}
  \caption{Geocomputation in relation to other fields}
  \label{fig:geocomputation}
\end{figure}

Geocomputation can be seen as an applied field of the more general field of computer graphics. 
Other applications of computer graphics include computer aided design (CAD), Building Information Modelling (BIM), molecular biology, medical imaging, robotics, but also special effects, video games, and graphic design.
For this reason, these applied fields can be regarded as neighbors to geocomputation \& GIS. 

It is important to recognize that certain geocomputational procedures fully overlap with computer graphics and these neighboring fields, while others are very specific to the field of GIS and geo-informatics.
As an example, matrix transformations and reprojections are commonplace in the wider field of computer graphics, but transformations formalized and structured in the form of \ac*{crs} is very specific to the field of GIS (See \reffig{fig:geocomputation}).
As such, geocomputational procedures can roughly be categorized in two fields: 
\begin{itemize}[-]
  \item geocomputations \emph{specific} to \ac{gis},
  \item \emph{common} computational geometry procedures
\end{itemize}
This categorization can be identified by noting if an operation appears in a neighboring field, or just in the field of \ac{gis}.

This \emph{specific} category exists because of \ac{gis}s foundation in the field of Geodesy, and the nature of geographical data. 
Geodata differentiates itself from any form of data by its sizable nature, and geospatial nature. 
GIS dataset sizes easily scale into terabytes of data, and each datum specifically represent a real, measured, earthly phenomenon.
This makes storage and accuracy much more relevant than other computer graphics applications. 

% NOTE TO SELF: GIVE CATEGORIES OF GEOCOMPUTATION IF IT IS ASKED FOR, OTHERWISE, LEAVE IT
% \subsection{Categories of Geocomputation}

% Geocomputation is a very broadly defined phenomenon, used to represent a great variety of computations. 
% To give an overview of this variety, a hierarchical subdivision of different types of geocomputation can be given, based on a subdivision of geodata types.

% The following distinctions are made between different geodata types:
% \begin{itemize}
%   \item Uniform
%   \subitem Rasters (Imagery)
%   \subitem Hexagons
%   \item Irregular, Vector-based
%   \subitem TIN 
%   \subitem solids
%   \subitem 3D Tiles
%   \item Semantic geodata:
%   \subitem Tabular geodata (QGIS)
%   \subitem Hierarchical, 'object oriented' geodata (GML / JSON) 
%   \item Point-cloud
% \end{itemize}

% Corresponding geocomputations are typically grouped together with one of these types of data. 
% However, this taxonomy is not perfect, since many computations exist \emph{between} between two different types of geodata.

% \subsubsection{Raster Geocomputation}

% - image processing
% - transformation kernels 

% \subsubsection{Vector Geocomputation}


% \subsubsection{Semantic Geocomputation}
% \begin{enumerate}[-]
%   \item often raster or vector at the core, with semantics layered on top 
% \end{enumerate}

% - 
% - 

% \subsubsection{Pointcloud Geocomputation}

\subsection{Geocomputation libraries}

\begin{figure}
  \centering
  \graphicspath{ {../../assets/images/background} }
  \includegraphics[width=380px]{all-geo-libraries-explained.jpg}
  \caption{Dependency graph of common geocomputation libraries. (This needs validation) }
  \label{fig:geolib-dependencies}
\end{figure}

Numerous geocomputational software libraries exist, written in a plethora of programming languages. 
Still, a certain 'Canon' can be defined based on popularity in terms of numbers of downloads, numbers of contributors, and number of dependent projects.  

Out of all open-source geocomputational libraries, the ones developed and maintained by the \ac{osgeo} (Source) can be regarded as the most significant to the field of \ac{gis}. 
These libraries include:
\begin{itemize}[]
  \item The \ac{gdal} (Source) 
  \item The Cartographic Projections and Coordinate Transformations Library titled PROJ (Source)
  \item The Geometry Engine Open Source (GEOS) (Source)
\end{itemize}
The geocomputations found in these libraries operate primarily on 2D / 2.5D raster and Simple Feature datasets (Source).
How these projects related together is presented by \reffig{fig:geolib-dependencies}.
Together, these libraries represent the core computational needs specific to the field of \ac{gis}. 

For the more 'common' computational geometry needs, the \ac{cgal} library is also widely used.

Almost all popular GIS end-user applications have these libraries at their core, such as 

\subsection{Conclusion}
The Take-away: 

Combination of general computational geometry, and computations very specific to GIS.

Geocomputation is mostly facilitated by a very select number of C++ libraries. 

\section{Frontend web applications (WEB) }
\label{sec:background-web}

This section offers a background on frontend web applications.
Since this topic is too large in scope to fully cover, three elements are chosen which are highly relevant to this study.
Finally, these three topics are linked to each other in \refsec{sec:background-web-conclusion}.

\subsection{Distributed systems}
\label{sec:background-web-terminology}

In software development of distributed systems, the following phrases exist: 
\begin{enumerate}[-]
  \item Client and Server 
  \item Frontend and backend
  \item Native application and web application
  \item web application and website
\end{enumerate}
While these phrases do overlap to an extend, years of interchangeable usage have lead to their differences often being overlooked. 
This study wishes to shed light on the relationships between these phenomena, as the nuances between them are vital to this studies contribution.

\subsubsection*{The client-server model}

\begin{figure}
  \centering
  \graphicspath{ {../../assets/images/misc/} }
  \includegraphics[width=380px]{todo.jpg}
  \caption{A typical client-server interaction) }
  \label{fig:client-server}
\end{figure}

First, the client-server model. 
The client-server model refers to a distributed application architecture which balances storage and processing responsibilities between two cooperating types of programs: 
Clients and servers.
In this model, a client sends a request to a server, and the server provides the response asked for (see \reffig{fig:client-server}).
While this model immediately invokes images of web clients and web servers, it is important to recognize that the client-server model is far older than either web applications, or the World Wide Web in general. 
It is an abstract computational model, of which the World Wide Web is just one example.
A corresponding client and server may even exist on the same machine. 
A program running on a machine can act as a client, a server, or both, based on the role this program sets out to fulfill in relationship to other programs. 

The client-server model is beneficial for sharing resources, both in terms of storage and processing. 
A distinction is made between centralized models, in which the bulk of these resources are centralized on one or more servers, and decentralized models, which distribute and offload some or all of the computational resources to the clients. 
A centralized model has the advantage of making clients simple and interchangeable, at the cost of making them highly reliant on the uptake of and connection to the server. This also generates more client-server traffic. 
A decentralized model makes clients independent and decreases traffic, at the cost of the complications caused by decentralized architectures. 
The choice between a centralized or decentralized client-server model is therefore highly reliant on the resources of client and server hardware, as well as the quality of the connection between the server and client.  

\subsubsection*{Frontend and backend}

The terms frontend and backend, though closely related to clients and servers, refer to different phenomenon. 
Both are separations of concerns, a design principle prevalent in computer science to specialize a program into separate responsibilities. 
However, client and server programs are defined by their separation into "requester" and "responder" roles, whereas the frontend and backend are defined by their separation into "presentation" and "data access" functions. 
Presentation functions are responsible for interacting with the end-user of the application, and is concerned with aspects such as user interface, user interaction, and rendering.
"data access" interacts with the physical hardware of the machine, and is concerned with aspects such as storage methods, database management, and scalability.  
It just so happens that the presentation functions often corresponds with a requester role,
and that data access functions often corresponds with the responder role.
However, this is never a given. 
A server can be responsible for providing both the frontend and backend functionality, in the case the presentation of an application is rendered on this server. 

\subsubsection*{Native application and web application}

The nuances between a web application and a native application must also be specified, alongside their relationship to clients and servers. 
In this context, we make a distinction between \emph{Programs}, which refer to individual processes on either the side of the client or the server, and an \emph{application}, which either represents a non-distributed, self contained program, or represents the whole of corresponding client and server programs together.
Client programs or server programs are also often abbreviated as clients and servers.  
If a client runs without the corresponding server it relies upon, we can say that the client functions, but the entire \emph{application} does not function. 
In practice, however, the term 'web app' often specifically refers to the client, confusingly enough.

In any case, a program is considered native if it directly runs on the operating system of a device.
A program is considered web-based or browser-based if a browser is required to run it. 
A web application always has a client, as it will always need to be initially served by a corresponding server.
However, the extend to which the functionality of a web app is self contained or continuously reliant on this server may vary.
A native program may also be a client, as there is nothing preventing a native program of making the exact same web request as a web application. 

Due to this ambiguity of 'client-side' being able to refer to both native and web clients, this study makes use of the terminology 'browser-based programs' or 'browser-based applications', to point to web clients in particular. 

Web applications have specific advantages and disadvantages compared to native applications. 
The big advantages are that web applications are cross-platform by nature, and offer ease of accessibility, since no installment or app-store interaction is required to run or update the app (src: vpl 2019, src: hybrid).
As soon as a web app is found, it can theoretically be used.
The containerized nature of the web also makes web applications in general more safe. 
For unknown native applications there is always a danger of installing malicious software, whereas an unknown web application without any privileges is practically harmless. 
The ability to share the a functional application with a link, or to embed it within the larger context of a webpage, is also not a trivial advantage.

The disadvantage is that normally, web applications can only be written in one, very-high level programming language: JavaScript. 
Its high-level nature leads to imprecision in using computational resources. 
For example, it makes no distinction between integer and floating point arithmetic.  
Additionally, the safety and containerization demands of the web make web applications more removed from the operating system and hardware.
Any type of \ac{os} interaction such as opening a window, interacting with the file system, or drawing directly to the screen buffer, is off-limits.  
Both these layers of indirection makes web applications traditionally unfavorable for demanding, highly specialized programs. 

\subsubsection*{web application and website}

Lastly, a soft distinction is also made between websites, and web applications. 
Roughly speaking, a web application is a website which requires javascript in order to be functional.
This makes websites more static, and web applications more dynamic, being able to change based on user input.  
Wikipedia (Source) can be considered a website, whereas overleaf (Source) is definitively a web application. 
Many border cases also exist, like Twitter (Source).
Following the above definition, twitter is a web application, despite the fact that its core functionalities could be implemented without any client-side javascript.

\begin{note}
  Sources: 

  (https://en.wikipedia.org/wiki/Web_application, i know i know, bad source, but this is more 'conventional wisdom' than true 'knowledge', couldnt find a more credible source, of what would make a person credible on this content)
  
  (https://en.wikipedia.org/wiki/Frontend_and_backend)
\end{note}


\subsection{Rich Clients}
\label{sec:background-web-rich}

In the early days of the World Wide Web, web applications were practically impossible, and the web consisted of websites exclusively. 
Then, with the introduction of the javascript scripting language in 199x (Source), and browser plugins like Adobe Flash (Source), the first couple of web application slowly started to be developed. 
Still, these early web applications exclusively used a centralized client-server model.
The clients were simple, and completely reliant on the server. 

In the decades that followed, the javascript runtime of web browsers saw continuous improvements, alongside additions like HTML5, facilitating more interactive usage of webpages.
As the web and web technologies matures, new ways of using these technologies are discovered.
Web applications became more interactive, and frontend functionalities were slowly moved from the server to the client. 

These developments have not stopped. Since 2012, a trend of \textbf{rich web-clients} can be wide recognized \cite{hamilton_client-side_2014, panidi_hybrid_2015, kulawiak_analysis_2019}.
At that point, the browser had become powerful enough to allow for decentralized client-server models.
By making servers nothing more than static file servers, and adding all routing and rendering responsibilities to the client, the interactivity of a web application could be maximized. 
This model was dubbed "single page application", and was and still is facilitated by javascript frameworks like Angular, React and Vue.
However, the real facilitator of these developments are the browsers themselves.
As the overhead created by these features would not be possible if javascript ran the way it did in 200x. 

This growth of the web application also lead to web applications being used 'natively'. 
Tools like Electron (Source) allow web applications to be installed and 'run' on native machines by rendering them inside of a stripped down browser. 
Many contemporary 'native' applications work like this, such as VS Code, Slack, and Discord.
Additionally, tools like React Native (Source) are able to compile a web application into a native application without a browser runtime.  

If the applications resulting from both types of tools are to be regarded as 'web apps' or 'native apps', is left as an exercise to the reader. 
In any case, it becomes clear that rich web clients and their build tooling are starting to blur the line between native and web software.

\subsection{WebAssembly}
\label{sec:background-wasm}

If the line between web application and native application was already starting to get blurry, WebAssembly makes this line almost invisible. 
From all browser-based features, WebAssembly turned out to be a deciding factor of this study. This requires us to be aware of the state of WebAssembly and its performance considerations.

\ac{wasm} is officially dubbed the fourth type of programming language supported by all major web browsers, next to HTML, CSS, and JavaScript.
Strictly speaking however, WebAssembly not a language, but a binary instruction format for a stack-based virtual machine.
(SOURCE: https://webassembly.org/)
it can be used to, theoretically, run any application or library in a web browser, regardless of the language used to create it, be it C/C++, Python, C\#, Java, or Rust. 
This means that in order to create a web application, developers can now in principle develop a normal, native application instead, which can then be compiled to WebAssembly, and served on the web just like any other web application. 

\subsubsection*{Limitations}

The sentence above uses the phrase \emph{in principle}, since there are quite a few caveats to the format. 
While in theory any application can be compiled to WebAssembly, in practice, not all applications turn into functional webassembly applications, due to certain factors.
These limitations can be split up into two groups: 
Limitations due to the web platform, and limitations due to the current state of the language and its host.

First of all, WebAssembly is required to adhere to the same containerization restrictions as javascript and the web at large. 
There is no '\m{os}' or '\m{sys}' it can call out to, as it cannot ask for resources which could be a potential security risk, like the file system.
Secondly, WebAssembly is in its early phases as a language, and is intended as a simple, bare-bones, low-level compile target. 
For example, the current version does not support concurrency features like multithreading.

Many of these shortcomings can be mitigated by calling JavaScript and HTML5 features from WebAssembly. 
This is what the majority of current WebAssembly projects look like. 
However, this layer of javascript 'boilerplate' or 'glue code' is inefficient, as it leads to duplication and redirection.
Additionally, platforms wishing to support WebAssembly must now also support javascript. 

\subsubsection*{Performance}

The initial performance benchmarks look promising. The majority of performance comparisons show that WebAssembly only takes 10\% longer than the native binary it was compared to \cite{haas_bringing_2017}. A later study confirms this by reproducing these benchmarks \cite{jangda_not_2019}. It even notices that improvements have been made in the two years between the studies. However, Jangda et. al. criticize the methodology of these benchmarks, stating that only small scale, scientific operations where benchmarked, each containing only 100 lines of code. The paper then continues to show WebAssembly is much more inefficient and inconsistent when it comes to larger applications which use IO operations and contain less-optimized code. These applications turn out to be up to twice as slow compared to native, according to their own, custom benchmarks. 
Jangda et. al. reason that some of this performance difference will disappear the more mature and adopted WebAssembly becomes, but state that WebAssembly has some unavoidable performance penalties as well. 
One of these penalties is the extra translation step, shown in \reffig{fig:wasm-trajectory}, which is indeed unavoidable when utilizing an in-between compilation target. 

Some studies have taken place evaluating \ac{wasm}'s performance for geospatial operations specifically. 
Melch performed extensive benchmarks on polygon simplification algorithms written in both javascript and WebAssembly \cite{melch_performance_2019}. 
It concludes by showing WebAssembly was not always faster, but considerably more consistent. 
Melch had this to say: "To call the WebAssembly code the coordinates will first have to be stored in a linear memory object. 
With short run times this overhead can exceed the performance gain through WebAssembly. 
The pure algorithm run time was always shorter with WebAssembly.". 
These findings match \cite{jangda_not_2019}, showing that the duplication of data into the webassembly buffer is a considerable bottleneck.

A recent study concerned with watershed delineation \cite{sit_optimized_2019} also concluded client-side WebAssembly to be more performant than server-side C, which, as a side effect, enabled their application to be published on the web without an active server. 

Lastly, the sparse matrix research of Sandhu et al. will be mentioned. \cite{sandhu_sparse_2018}. It shows again that WebAssembly's performance gain is most notable when performing scientific computations. it states: "For JavaScript, we observed that the best performing browser demonstrated a slowdown of only 2.2x to 5.8x versus C. Somewhat surprisingly, for WebAssembly, we observed similar or better performance as compared to C, for the best performing browser.". It also shows how certain preconceptions must be disregarded during research. For example, it turned out that for WebAssembly and JavaScript, double-precision arithmetic was more performant than single-precision.

Even though this proposed study falls in the category of scientific computation, these performance considerations will still have to be taken into account. The most important conclusion to to take away from prior research on WebAssembly is that \ac{wasm} must not be regarded as a 'drop-in replacement', as \cite{melch_performance_2019} puts it. Just like any language, WebAssembly has strengths and weaknesses. While \ac{wasm} is designed to be as unassumptious and unopinionated about its source language as possible, the implementations of host environments do favor certain programming patterns and data structures over others, and this will have to be taken into account when using the compile target.

\begin{figure}[!tbp]
  \centering
  \begin{minipage}[b]{0.80\textwidth}
    \graphicspath{ {../../assets/images/misc/} }
    \includegraphics[width=300px]{todo.jpg}
    \caption{Comparison of compilation trajectories}
    % based on the finding of \cite{jangda_not_2019}
    \label{fig:wasm-trajectory}
  \end{minipage}
\end{figure}

% \subsubsection{Background}

% The original paper on WebAssembly was published on June 14, 2017 \cite{haas_bringing_2017}. The authors write that the reason behind the creation of WebAssembly is the observation that certain web applications started using JavaScript as a compile target, using a high-performance subset of JavaScript called 'asm.js' \cite{mozilla_asmjs_2013}. However, JavaScript remains a high-level, highly abstract programming language, which never intended to be used as a compile target. The discrepancy between intended use and actual use led to many complications for developers using JavaScript this way, but also for the developers of JavaScript itself \cite{haas_bringing_2017}. 
% In order to relieve javascript of the responsibility of being a 'low-level' compilation target, developers of the four major browser vendors Mozilla, Google, Apple and Microsoft created WebAssembly and its corresponding paper, in a joined effort.

% This paper starts by promising WebAssembly as a save, fast, portable and compact compilation target. It continues by showing how previous attempts at low-level code on the web fail in at least one of these criteria, and that WebAssembly is the first to deliver on all of them. The follow up chapters cover a proof of memory safety, a proof of soundness of the language design, and the design decisions which had to be made to live up to those four criteria. These details will become relevant to the proposed thesis when reasoning about why WebAssembly might be faster in one case versus another.

% \subsubsection*{Adoption \& Implementation}

% not in a vaccuum

% On 5 December 2019, the \ac{w3c} officially pronounced WebAssembly as the fourth programming language of the web \cite{w3c_world_2019}. Philippe Le Hégaret, the \ac{w3c} Project Lead, writes “The arrival of WebAssembly expands the range of applications that can be achieved by simply using Open Web Platform technologies. In a world where machine learning and Artificial Intelligence become more and more common, it is important to enable high performance applications on the Web, without compromising the safety of the users,”. Since then, most major browsers have added official WebAssembly support.

% As of writing this proposal, WebAssembly has of yet not seen widespread adoption in web developer communities. Opinions deviate, but in general, WebAssembly is considered a niche technology, often being named as 'experimental' and 'bleeding edge'. 

% This would explain why, to the best of the author's knowledge, not many projects and papers explicitly link WebAssembly and GIS. Papers on \ac{wasm} do state \textit{"3d data transformations and visualization"} as some of the examples of a high performance web applications \cite{haas_bringing_2017, jangda_not_2019}. What's more, certain GIS applications, like Google Earth, have started to use WebAssembly, as seen in \reffig{fig:google-earth} \cite{google_google_2020}. How it is used is unknown due to the engine being closed-source, but it is speculated that \ac{wasm} is used to access code written for the original C++-based desktop application.

% \begin{figure}[!tbp]
%   \centering
%   \begin{minipage}[b]{0.80\textwidth}
%     \includegraphics[width=\textwidth]{../images/google-earth-uses-webassembly.PNG}
%     \caption{Google Earth utilizing WebAssembly. Source: \cite{google_google_2020}}
%     \label{fig:google-earth}
%   \end{minipage}
% \end{figure}


% On the topic of WebAssembly, the most important conclusion to to take away from prior research is \ac{wasm} must not be regarded as a 'drop-in replacement', as \cite{melch_performance_2019} puts it. Just like any language, WebAssembly has strengths and weaknesses. While \ac{wasm} is designed to be as unassumptious and unopinionated about its source language as possible, the implementations of host environments do favor certain programming patterns and data structures over others, and this will have to be taken into account during the proposed study.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Based on the studies on WebAssembly, we can conclude that the compilation peculiarities of WebAssembly have to be taken into account, as it cannot be regarded as a 'drop in replacement'. There is also a significant difference between using WebAssembly theoretically, and using it realistically. The studies on Client-side geoprocessing tell us that these implementation details can have vast consequences on user experience, and studies on the Geoweb express that this user experience is vital to FAIR, cross-community geoprocessing.

% What this means for the methodology, is that a significant portion of this study's attention will have to go to experimenting with different ways of compiling to WebAssembly, while making sure it can still be used in a realistic scenario.
% If it turns out that the use-case app can only be used by experienced end-users who take special \ac{wasm} considerations in mind, a big reason of using the web, namely its accessibility, would be lost.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsubsection{(More on webassembly)}

% Not just open source: process sharing using fully containerized instances. Think .

% current vision / direction: containerized, sharable processes, together with web-based, front end visual programming environments ( RasterFoundry). Docker is usually named as a vision for these sharable processes.

% \m{->} We do have examples of cloud-native geodata formats, and some examples of cloud-based geo-computation (RasterFoundry , Google Earth Engine, more). However, these approaches have not yet tried to use truly sharable, containerized geoprocesses using Docker or WebAssembly. 

% \m{->} WebAssembly as a whole is underresearched. WebAssembly is not a fully virtualized container image, but just a binary set of instructions, meant to be executed on a virtual machine. Think of safe, cross-platform dll's. 
% WebAssembly is in this regard more simple than docker, but this gives it more opportunities. 
% WebAssembly runs in the browser for instance. 

% \m{->} This opportunity to run in the browser would enable these cloud-native frontend environments to "dry-run" these processes from within the browser, completely detached from the server, as a means to experiment with processes on a small scale before applying them to a cloud native environment. 

% \m{->} However, no implementations exist yet which combines containerized processes with these frontend computation environments. 

% # 2. BACKGROUND

% ## 2.1 The Web Browser & JavaScript
% -  main players (chrome, safari, firefox, edge(==chrome))
% - The browser js speed armsrace
% - How that lead to WebAssembly

% ## 2.2 The Geospatial Web. 
% [Still relevant]

% 2 biggest reasons against client-side geoprocessing: 
% - not performant enough
% - no equivalent to industry-standard libraries (CGAL / GDAL). 

% WebAssembly COULD solve both, so this study includes WebAssembly as 


% <br><br>

% .....

\subsection{Conclusion}
\label{sec:background-web-conclusion}

When reading \refsec{sec:background-web-terminology}, \refsec{sec:background-web-rich}, and \refsec{sec:background-wasm} together, a pattern emerges. 
WebAssembly blurs the line between the web-based and native development even further than the rich clients, and invites a further re-examination of our established models of distributed systems.
The compile target allows web-apps to make use of native libraries, and allows native software to be run on the web.
This second aspect offers a complete reverse workflow compared to the now popular Electron based applications described in \refsec{sec:background-web-rich}.
 
There was a significant delay between the improvements of the browser, and the widespread popularity of rich web clients. 
This study argues that as of right now, we are in the middle of a similar situation. 
A new technologies exist, it is implemented by all major browsers, and offers completely new ways of working with the web platform as a whole. 
The question remains what this will mean for the established models of clients and servers, the frontend and backend, and the web and native contexts. 

\section{Visual Programmming}
\label{sec:background-vpl}

The third body of work this study draws from is works on the topic of visual programming. 
This section offers a brief overview on the topic itself, after which since \refsec{sec:related-geovpl} and \refsec{sec:related-webvpl} cover uses of visual programming in geocomputation and on the web, respectively. 

\subsection*{Visual programming languages}

A \ac{vpl}, or visual programming environment, is a type of programming language represented in a graphical, non-textual manner.
A VPL often refers to both the language and the \ac{ide} which presents this language in an editable way, by means of a \ac{gui}.
A visual programming language allows users to create programs by adding preconfigured components to a canvas, and connecting these components to form programs. 

Multiple types of \ac{vpl}s exist, but also multiple taxonomies of these types.
This study bases itself on the classifications presented in \cite{kuhail_characterizing_2021}, stating four different types of visual programming languages: 
\begin{enumerate}
  \item \textbf{Block-based languages}, in which all normal programming language features, like brackets, are represented by specific blocks which can be 'snapped' together (\label{fig:sidebyside:1}).
  \item \textbf{Diagram-based languages}, in which programming functions or operations are represented by components, and variables are represented by links or 'cables' between these components. This makes the entire program a \ac{DAG}.
  \item \textbf{Form-based languages}, in which the functioning of a program can be configured by means of normal graphical forms. 
  This approach enhances the stability and predictiveness compared to other types, at the cost of expressiveness.
  \item \textbf{Icon-based languages}, in which users are asked to define their programs by chaining highly abstract, iconified procedures. 
\end{enumerate}

The meta analysis of \cite{kuhail_characterizing_2021} shows a great preference among researchers for block- and diagram-based languages. 
Only 4 out of 30 of the analyzed articles chose a form-based vpl, and only 2 chose an icon-based approach.  

\begin{figure}
\centering
\begin{subfigure}[b]{0.45\linewidth}
  \graphicspath{{../../assets/images/background/vpl/}}
  \centering
  \includegraphics[width=\linewidth]{block-based.png}
  \caption{}\label{fig:vpl-types:1}
\end{subfigure}%
\qquad %-- that adds some space between th 2 figures
\begin{subfigure}[b]{0.45\linewidth}
  \graphicspath{{../../assets/images/background/vpl/}}
  \centering
  \includegraphics[width=\linewidth]{diagram-based.jpg}
  \caption{}\label{fig:vpl-types:2}
\end{subfigure}%
\\
\begin{subfigure}[c]{0.45\linewidth}
  \centering
  \graphicspath{{../../assets/images/background/vpl/}}
  \includegraphics[width=\linewidth]{form-based.png}
  \caption{}\label{fig:vpl-types:3}
\end{subfigure}%
\qquad %-- that adds some space between th 2 figures
\begin{subfigure}[d]{0.45\linewidth}
  \centering
  \graphicspath{{../../assets/images/background/vpl/}}
  \includegraphics[width=\linewidth]{icon-based.jpg}
  \caption{}\label{fig:vpl-types:4}
\end{subfigure}%
\caption[Types of \ac{vpl}s]{Four different types of visual programming languages: Block-based, diagram-based, form-based, and icon-based, respectively}%
\label{fig:vpl-types}
\end{figure}

Visual programming languages are used in numerous domains. 
The vpls of the 30 studies examined by (SOURCE) were aimed at domains such as the Internet of Things, robotics, mobile application development, and augmented reality. 
Within the domain of systems control and engineering, The Ladder Diagram vpl (SOURCE) is the industry-standard for programming Programmable Logic Controllers (PLCs).
\ac{vpl}s are also widely used within computer graphics related applications, including the field of \ac{gis}, 
These will be covered in \refsec{sec:related-geovpl}.
Lastly, \ac{vpl}s also have great educational applications. 
Harvard's introduction to computer science course, CS50, famously starts out with Scratch, a block-based visual programming language, mostly targeted at children to teach the basics of computational thinking. 

% \begin{note}
%   Traditionally, visual programming has been successfully used to help novices
%   learn basics of programming by visualizing elements of a program. 
%  However, visual programming is increasingly being used by end 
%  users in various domains to create and tailor applications that are useful 
%  beyond the realm of education. 
%  For instance, VPLs are now being used in fields such as 
%  the 
%  From characterizing_2021
 

\subsection{Advantage: Usability}

Studies on \ac{vpl}s indicate that generally speaking, VPLs make it easy for end users to visualize the logic of a program, and that vpls eliminate the burden of handling syntactical errors \cite{kuhail_characterizing_2021}.

The locally famous Cognitive Dimentions study \cite{green_usability_1996}, state that \emph{"The construction of programs is probably easier in VPLs than in textual languages, for several reasons: 
there are fewer syntactic planning goals to be met, such as paired delimiters, discontinuous constructs, separators, or initializations of variables; 
higher-level operators reduce the need for awkward combinations of primitives; 
and the order of activity is freer, so that programmers can proceed as seems best in putting the pieces of a program together."}. 
Indeed, a vpl UI can be used to eliminate whole classes of errors on a UI level by, for example, not allowing the connection of two incompatible data types. 

\begin{note}
  TODO: more stuff here. This needs to be more nuanced
\end{note}

% //The meta analysis of \cite{kuhail_characterizing_2021} also 

These properties together make visual programming also highly suitable for activities of \textbf{experimentation} and \textbf{debugability}, and not only for end users. 

\subsection*{Advantage: End User Development \& Low Coding}
a \ac{vpl} done right can make automation available to a very large audience, and this is exactly the point. 
Visual Programming is part of a larger field, named End User Development (eud). 
The field is concerned with allowing end users who are not professional software developers to write software applications, using specialized tools and activities. 
This however, does not mean that experienced developers have nothing to gain from this research. 
Lowering the cognitive load of certain types of software development could save time and energy which can then be spend on more worthwhile and demanding tasks. 

\cite{kuhail_characterizing_2021} point out two serious advantages of EUD. 
First, end users know their own domain and needs better than anyone else, and are often aware of specificities in their respective contexts (\cite{kuhail_characterizing_2021}). 
And two, end users outnumber developers with formal training at least by a factor of 30-to-1 according to Kuhail et. al., and my suspicion is that this might be much higher.

% rewrite this more professionally...
I would like to add that offering the general public a chance to automate repetitious workflows might not only increase productivity, but can also greatly improve the quality of life in general, by focussing on the profound instead of the mundane. 

In the private sector, \ac{eud} is represented by the "low code" industry. 
Technology firms such as Google and Amazon are investing at scale in low-coding platforms (\cite{kuhail_characterizing_2021}),
The market value was estimated at 12.500 Million USD (Source: ), and with a growth rate between 20 and 40 percent, the value may reach as high as 19 Billion by 2030 (SOURCE: FORRESTER) 

\subsection{Advantage: Dataflow programming}
\label{sec:background:dataflow}
An important aspect of especially diagram-based visual programming is its closeness to the field of dataflow programming. 

Dataflow programming is a programming paradigm which internally, represents a program as a \ac{dag} (SOURCE Dataflow 2012). 
A graphical, editable representation of a dataflow program would result into a diagram-based \ac{vpl}.

The big computational advantage of this model, is that it allows for implicit concurrently (SOURCE dataflow 2012). 
In other words, every node of a program written using dataflow programming can be executed in isolation of any other nodes, as long as the direct dependencies (the inputs) are met. 
No global state or hidden side effects means no data-race issues, which allows parallel execution of the program by default.
When using other paradigms, programmers need to manually spawn and manage threads to achieve the same effect. 

This leads into an interesting side-effect of using dataflow programming / a diagram-based \ac{vpl}: 
By only permitting pure, stateless functions with no side-effect, and only immutable variables, end users automatically adopt a functional programming style.
Functional programming has many benefits of its own besides concurrency, such as clear unit testing, hot code deployment, debugging advantages, and lending itself well for compile time optimizations (SOURCE: functional programming).


% functional programming

% Spreadsheets are probably the most common example of DFP
% and widely adopted by every type of computer users.
% On a spreadsheet, each cell represents a node that can either be an expression
% or a single value. Dependencies can exist to other cells. Following the dataflow
% model, whenever a cell gets updated, it sends its new value to those who depend
% on it, that update themselves before also propagating their new values. This
% specific type of application is commonly denominated as Cell-Oriented DPF or
% Reactive programming.

The important take-away is this: 
A \ac{vpl} is not just a matter of a user-friendly UI or a stylistic choice.
This might be true for block-based vpls, but not for diagram-based vpls. 
By closely resembling dataflow itself, and because of its functional programming nature, diagram-based vpls can actually lead to faster and more reliable software.

\subsection{Disadvantages and open problems}
\label{sec:background:vpl:disadvantages}

As stated by (SOURCE: functional programming), there is no such thing as a free lunch. 
\ac{vpl}s and dataflow programming en large have got certain disadvantages and open problems.

(Dataflow programming is an area still open to further research, with some open
issues to answer. In fact, most of the open questions today have long been
identified and despite the improvements, patterns for answering them are yet to
be achieved.)

% block-based visual programming: just purely UI, diagram-based vpl add additional, functional programming qualities. 


\subsubsection*{Iteration and conditionals}

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.45\linewidth}
    \graphicspath{{../../assets/images/background/vpl/}}
    \centering
    \includegraphics[width=\linewidth]{iteration-vpl.png}
    \caption{}\label{fig:vpl-iteration:1}
  \end{subfigure}%
  \qquad %-- that adds some space between th 2 figures
  \begin{subfigure}[b]{0.45\linewidth}
    \graphicspath{{../../assets/images/background/vpl/}}
    \centering
    \includegraphics[width=\linewidth]{iteration-text.png}
    \caption{}\label{fig:vpl-iteration:2}
  \end{subfigure}%
  \caption[Comparrison of iteration]{A factorial function, written in a vpl, and textual form}%
  \label{fig:vpl-iteration}
  \end{figure}

A problem described in almost all reviewed vpl literature (Source: advanced in dataflow, SOURCE: Dataflow programming,  SOURCE: COGNITIVE), is that the \ac{dag} model of diagram-based vpls are ill-suited for representing even the most basic flow control statements: \m{if, else, for, while}.
Even if the acyclic quality of the dataflow graph is omitted, the resulting models are significantly more complicated compared to their textual counterparts, as shown by \reffig{fig:vpl-iteration}.

\subsubsection*{Encapsulation \& reusability}
Similar and yet different is the topic of encapsulation, or, how (SOURCE: COGNITIVE) names this problem: 'visibility'.
It is widely known that as a program scales in size, the complexity of handling the application scales exponentially.
In textual languages, reducing this complexity is often achieved by means of encapsulating sub-routines and re-usable parts of the program into separate functions.
Inner functionality is then hidden, and operations can be performed on a higher level of abstraction. 
This hierarchy of abstraction is just as achievable for \ac{vpl}s as described by (SOURCE: Dataflow programming).
However only a select number of \ac{vpl}s offer a form of encapsulation, and even less allow the creation of reusable functions, or creating reusable libraries from vpl scripts.
It appears that \ac{vpl} researches and developers are either not aware of the importance of encapsulation, or have encountered problems in representing this feature in a graphical manner.

% \subsubsection{High viscocsity}
% Viscosity was surprisingly high in the languages we looked at. The role of the diagram editor is crucial, yet few research papers in the usual programming literature discuss the design of effective diagram editors. In our straw viscosity test we found a range from about 1 minute to about 9 minutes for making semantically equivalent changes to programs in different languages. Visibility can be very poor. Systematic, easy-to-understand search tools need to be developed and user-tested, and if at all possible de facto standards should be adopted.

\subsubsection*{Subjective Assessment}
Additionally, the claims that \ac{vpl}s lend themselves well for end-user development is problematic from a technical perspective. 
Usability is a nebulous phenomenon, and challenging to measure empirically.
As often with more subjective matter, researchers have yet to form a consensus over a general evaluation framework. 
There is, however, a reasonable consensus on the 'qualities' a VPL should aspire to. 
This is different from a full assessment framework, but nontheless useful for comparing \ac{vpl}s.
The dimensions given in the cognitive dimensions framework (SOURCE) have acquired a somewhat canonical nature within \ac{vpl} research.
The number of citations of this work is relatively high, and indeed, almost all \ac{vpl} studies the author was able to find referred back to this critical work.   
In so far as this study needs to address the usability of the prototype VPL, we will thus follow this consensus, and base any assessment on (SOURCE).

% - we will make no such attempt. usability serves as background motivation. 
% - this study assumes vpl's are 'in general' more usable to end-users 
% than text-based alternatives, based on the positive results of most of the 
% studies analysed by communicating_2021.

\subsubsection*{Poor life-cycle support}
Finally, (communicating 2021) names the 'life cycle' of applications created by \ac{vpl}s as one of the most overlooked aspects within VPL research.
Out of the 30 studies covered by the meta analysis, only one briefly touched the topic of life cycle. 
Life cycle in this context refers to all other activities besides "creating an application that does what it needs to do".
Examples of these activities are version control, extending an existing application, debugging, testing the codebase, and publishing the application to be used outside of an \ac{ide}. 
These operational aspects are critical to making any application succeed, and \ac{eud} research should not be limited to purely the aspect of creating functionalities.

This oversight on life-cycle aspects can be found in the \ac{vpl} \& low-coding industry as well. 
\begin{note}
TODO: Formalize these ramblings, or don't. 

- no existing VPL ( that I know of ) has proper git-based version control. 
  - Most vpls use proprietary storage and collaboration methods. 
- vpls which are able to compile to a textual format are rare
  - vpls able to compile to a programming language, or a headless format, are even more rare. 

- Let alone: on operational programming paradigms such as Test driven development (TDD), continuous integration (CI), continuous delivery (CD).  

- in general, the life-cycle support of most vpls found in the low-coding industry are closely tied to their business models. 
  - Users can only use the publication tools, version control tools, and package / library managers offered to them by the vendor.

\end{note}

And while we are on the topic of publication, only 16 out of 30 of the tools analysed by (communicating 2021) were available publicly with some documentation.
It seems the lack of publication tooling might also partially be due to a lack of publication in general. 


\subsection{Conclusion}

The background literature clearly indicates many advantageous properties of vpls, both in terms of (end) user experience and the dataflow programming properties. 
Additionally, the studies showed important considerations which have to be taken into account in the design of any vpl.  
Lastly, the studies agree on several open-ended issues of which a satisfying answer is yet to be found. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Related works}

This chapter offers a review of related and comparable studies.
while almost no projects or studies exist at the intersection of all three of these fields, we do find related studies which intersect two of these fields, represented by the edges of \reffig{fig:triangle-model}:
\begin{enumerate}[-]
  \item \refsec{sec:related-geoweb} reviews related works on browser-based geoprocessing
  \item \refsec{sec:related-geovpl} reviews related works on VPLs used for geo-computation
  \item \refsec{sec:related-webvpl} reviews related works on VPL web applications
\end{enumerate}


\section{Browser-based geocomputation}
\label{sec:related-geoweb}

This section is dedicated to related works on client-side geocomputation, or browser-based geocomputation. 
this study prefers to use "browser-based geocomputation" in order to circumvent the ambiguity between native clients like QGIS, and web clients, as described in \refsec{sec:background-web}.

First, a small paragraph on the motivation behind browser based geocomputation.
As stated in \refsec{sec:background-web}, web applications offer safety, distribution and accessibility advantages over native applications.
As such, browser based gis has become a sizable component of the geospatial software landscape. 
% For the average person, an interactive \ac{gis} web application is often their first and only exposure to such a system, be it a web mapping service, a navigation system, or a pandemic outbreak dashboard. 
However, despite the popularity of geographical web applications, the range of actual \ac{gis} abilities these applications have is generally speaking very limited. \ac{geocomputation} is usually not present within the same software environment as the web app. 
This limited range of capabilities inhibits the number of use cases geographical web applications can serve, and with that the usefulness of web \ac{gis} as a whole.
If web applications gain \ac{geocomputation} capabilities, they could grow to be just as diverse and useful as desktop \ac{gis} applications, with the added benefits of being a web application. It would allow for a new range of highly accessible and sharable geocomputation and analysis tools, which end-users could use to post-process and analyze geodata quickly, uniquely, and on demand.

This need has let to the field of \ac{bbg}.

% NEW PRACTICAL ATTEMPT. WHY? 
% - csg still has a lot of potential
% - previous studies:
%   - are dated
%   - prioritized theory over practicalities
%   - utilized the web's major feature of Accessibility and the clients feature of    
%     Interactivity inadequately 
%   - were not creative enough in terms of possible use-cases 

Browser-based geocomputation has seen some academic interest throughout the last decade \cite{hamilton_client-side_2014, panidi_hybrid_2015, kulawiak_analysis_2019}.

Hamilton et. al. created a 'thick-client', capable of replacing certain elements of server-side geoprocessing with browser-based geoprocessing \cite{hamilton_client-side_2014}. 
At first glance, the results seem unfavorable . 
The paper states how "the current implementation of web browsers are limited in their ability to execute JavaScript geoprocessing and not yet prepared to process data sizes larger than about 7,000 to 10,000 vertices before either prompting an unresponsive script warning in the browser or potentially losing the interest of the user."(SOURCE). 
While these findings are insightful, they are not directly applicable to the efforts of this study proposal. Three reasons for this:

\begin{itemize}
  \item The paper stems from 2014. Since then, web browsers have seen a significant increase in performance thanks to advancements in JavaScript JIT compilers \cite{haas_bringing_2017, kulawiak_analysis_2019}. 
  \item The paper does not use compile-time optimizations. The authors could have utilized 'asm.js' \cite{mozilla_asmjs_2013} which did exist at the time. 
  \item The paper uses a javascript library which was never designed to handle large datasets.
\end{itemize}

The same statements can be made about similar efforts of Panidi et. al. \cite{panidi_hybrid_2015}. 
However, Panidi et. al. never proposed browser-based geoprocessing as a replacement of server-side geoprocessing. 
Instead, the authors propose a hybrid approach, combining the advantages of server-side and browser-based geoprocessing. 
They also present the observation that browser-based versus server-side geoprocessing shouldn't necessarily be a compassion of performance. 
"User convenience" as they put it, might dictate the usage of browser-based geoprocessing in certain situations, despite speed considerations \cite{panidi_hybrid_2015}. 

This concern the general web community would label as \ac{ux}, is shared by a more recent paper \cite{kulawiak_analysis_2019}. 
Their article examines the current state of the web from the point of view of developing cost-effective Web-GIS applications for companies and institutions. 
Their research reaches a conclusion favorable towards browser-based data processing: "[Client-side data processing], in particular, shows new opportunities for cost optimization of Web-GIS development and deployment. 
The introduction of HTML5 has permitted for construction of platform-independent thick clients which offer data processing performance which under the right circumstances may be close to that of server-side solutions. 
In this context, institutions [...] should consider implementing Web-GIS with client-side data processing, which could result in cost savings without negative impacts on the user experience.".

% Based on the topic of client-side geospatial processing, we can state that web technologies contain a very dynamic temporal component. All research can become outdated, but performance analysis of web technologies are especially quick to change.  

From these papers we can summarize a true academic and even commercial interest in browser based geoprocessing over the last decade. 
However, practical implementation details remain highly experimental, or are simply not covered.  
The implementations of \cite{panidi_hybrid_2015, hamilton_client-side_2014} were written in a time before WebAssembly \& major javascript optimizations, and the study of \cite{kulawiak_analysis_2019} prioritized theory over practice. 
Additionally, to the best of the authors's knowledge, all papers concerned with browser-based geoprocessing either tried to use existing JavaScript libraries, or tried to write their own experimental WebAssembly / JavaScript libraries. 
No studies have been performed on the topic of compiling existing C++/Rust geoprocessing libraries to the web. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Commercial web-based geocomputations software}
Despite the earlier statement of the general lack of \ac{geocomputation} within browsers, there are exceptions. 
A select number of web-based \ac{gis} applications are starting to experiment with empowering end-users with geocomputation. 
These applications will briefly be mentioned.
 
% https://geotiff.io/
GeoTIFF (SOURCE, \reffig{fig:geotiff}), is a web-based geoTIFF processing tool, and fully open source. 
It offers basic operations such as taking the median or \& mean of a certain area, color band arithmetic, and can plot histograms, all calculated within the browser using customly written javascript libraries.

\begin{figure}
  \centering
  \graphicspath{ {../../assets/images/background/geo-web/} }
  \includegraphics[width=270px]{geotiff.png}
  \caption{The geoTIFF.io application}
  \label{fig:geotiff}
\end{figure}


\begin{figure}
  \centering
  \graphicspath{ {../../assets/images/background/geo-web/} }
  \includegraphics[width=270px]{rasterfoundry-2.jpg}
  \caption{The ModelLab application}
  \label{fig:modellab}
\end{figure}

% https://www.eclipse.org/community/eclipse_newsletter/2018/december/geotrellis.php 
% https://www.azavea.com/blog/2016/09/26/raster-foundry-model-lab-phase-ii-sbir/
Azavea's Raster Foundry and ModelLab (Powered by GeoTrellis), is another GeoTIFF / raster based web processing tool, in which basic queries and calculations are possible (SOURCE). 
This tool offers more advanced types of geocomputation, like buffering / minkowski sums, and even multi-stage processing via a simple but clear visual programming language (see \reffig{fig:modellab}). 
% There reasoning: "Widespread access to frequent, high-resolution Earth observation imagery has created the need for innovative tools like ModelLab that will 
However, the tool uses mostly server-side processing, making this application less relevant to this study. 

Also, despite their mission statement to: "help individuals and organizations to effectively access, analyze, edit, and visualize remotely sensed data in transformative new ways without years of specialized training or ongoing investments in proprietary software and technology infrastructure."(Source), The tool appears to be reliant on their own proprietary infrastructure to save and run the application.
The author of this study was also not able to find a public demo of the application. 

\begin{figure}
  \centering
  \graphicspath{ {../../assets/images/background/geo-web/} }
  \includegraphics[width=270px]{omnibase.png}
  \caption{The Omnibase application}
  \label{fig:omnibase}
\end{figure}

The last web-based geocomputation platform this study would like to mention is Geodelta's Omnibase application (SOURCE: GEODELTA, \reffig{fig:omnibase}). 
Omnibase is a 3D web \ac{gis} application for viewing and analyzing pointclouds and areal imagery datasets.
It offers client-side geocomputation in the form of measuring distances between locations, and calculating the area of a polygon.  
It also offers photogrammetry-techniques such as forward incision of a point in multiple images, but these are calculated server-side. 

% - https://openscad.org/


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%  \subsection*{The Cloud Native Geospatial movement}

% ( Not sure if I wanna go there... )

% % Establish OGC in two sentences, mentioning their name and Vision
% The Open Geospatial Consortium (OGC)...
% Mission: FAIR Geodata 

% % Establish Cloud Native movement.
% % GIS as one big LAN party
% A prominent development within the OGC is the recent effort towards a \textbf{"Cloud Native Geospatial"} future. 
% This initiative aims to radically simplify geodata storehouses to static servers serving large, singular binary geodata files. All processing and analysis of this geodata can then be performed by separate cloud-based web services. 
% This architecture has many advantages over current geodata storage and analysis methods:
% \begin{itemize}
%   \item These new Cloud Native geodata formats are much cheaper to access by front-end and back-end services, compared to active services.
%   \item Substituting active SQL or noSQL databases by static binary files is easier and cheaper for data providers, leading to more and more readily available geodata.
%   \item By using supercomputers (Microsoft Planetary Computer) and cloud-storage (AWS), Geodata processes could make use of near-infinite computational and storage resources. 
%   \item By having all data centralized in one location or type of location, new, large scale patterns within our geodata could be discovered.  
%   \item For web GIS, this would offer direct data streaming options, similar to services like "Netflix" or "Spotify".  
% \end{itemize}

% These features may have a far reaching impact on society. Chris Holmes, forerunner of the cloud-native geospatial movement, envisions what the movement could mean for even non-GIS users: 
% \emph{
%   With the introduction of accessible, centralized data, and the dramatically different workflows that follow, Cloud Native Geospatial has the potential to introduce new, non-specialized users to the power of geospatial information that GIS practitioners have enjoyed for decades. [...]. The ecosystem of geospatial experts will collaborate to create analyses and insight, but any non-expert user will be able to select and apply those to the geographic area they care about. \~ Chris Holmes
% }
% % This is also reflected by cloud-native based tools like (Google Earth Engine or RasterFoundry) may achieve such a feed, by being web based and stuff...
% All these reasons explain why the OGC and many other parties are now actively pursuing this vision.

% But while this vision is in active development, many large-scale challenges are still in its way. 
% One of the most important challenges is the required paradigm shift within geo-computation / geoprocessing workflows. 
% The current, common geo-computation workflow of retrieving online data, only to run it through a local process and send the resulting data back into servers, will have to be reversed: In a cloud-native future, we will not retrieve data for our local process, but we will upload our process to the data.  
% This introduces a sizable challenge: \textbf{Portable, Containerized Geo-computation}.

% % \textbf{and the algorithms powering the processing can be shared online and customized collaboratively}. -> Chris again

% % \begin{itemize}
% %   \item Up to this point, the world of GIS has done a considerable effort to make geodata more Findable, Accessible, Interoperable, and Reusable. The challenge of Portable geo-computation now forces us to extend the effort of FAIR geodata to FAIR geodata computation as well.  
% %   \item If we want our geodata processes to be just as portable as the geodata it takes as input, then perhaps the FAIR paradigm should extend from FAIR geodata to FAIR geodata processing . FAIR geo-computation.
% %   \item Furthermore, it remains a mystery how these containerized containerized processes will be configured and accessed by frontend computation environments. 
% %   \item Holmes: one of the vital ingredients: \emph{"and the algorithms powering the processing can be shared online and customized collaboratively"}.
% % \end{itemize}

% The challenge of sharing and chaining together containerized fragments of geoprocesses to a variety of environments will require more than just open source collaboration. 
% This study interprets the challenge of portable geo-computation by means of the FAIR paradigm. 
% If geodata processes need to be just as portable as the geodata forming the input and output, then perhaps the FAIR paradigm should extend from FAIR geodata to FAIR geodata \emph{processing} as well.
% The challenge facing the cloud-native vision then becomes: \textbf{How to make geo-computation Findable, Accessible, Interoperable, and Reusable?} 
% This links back to containerization, for containerization is a very powerful method of making geo-computation more Interoperable and Reusable.

% % state of the art regarding this issue, make a path towards the particular thesis, and why it is an application
% The current state of the art is far removed from either portable or FAIR geo-computation. 
% \todo{Improve this intro}
% \begin{itemize}
%   \item current methods: Docker, and some geo-computation platforms.
%   \item Not many implementations using WebAssembly, while this is a prime candidate: Even the guy who made Docker said so. 
%   \item ignore the cloud: focus on the act of containerizing geoprocesses using webassembly an sich
% \end{itemize}

\section{Visual programming and geocomputation}
\label{sec:related-geovpl}

This section is dedicated to all related works on \ac{vpl}s related to geocomputation.

\reffig{fig:geovpl:table} offers an overview of some of the more significant \ac{vpl}s present in not only \ac{gis}, but also the neighboring domains based on computer graphics.
Why these fields are regarded as neighbors are explained in \refsec{sec:background-geo}. 

\begin{figure}
  \centering
  \graphicspath{ {../../assets/tables/} }
  \includegraphics[width=400px]{geovpl.png}
  \caption{An overview of VPLs in the field of GIS and adjacent domains}
  \label{fig:geovpl:table}
\end{figure}

\subsection*{ VPLs in GIS }

Within the field of geo informatics, \ac{vpl}s are not a new phenomenon. VPLs have been used for decades to specify geodata transformations and performing spatial analyses.  

By far the most well-known visual programming language within the field of \ac{gis} is the commercial \ac{etl} tool FME (Source, \reffig{fig:gisvpl:1}). 
This tool is widely used by \ac{gis} professionals for extracting data from various sources, transforming data into a desired format, and then loading this data into a database, or just saving it locally.  
FME is most often used within GIS to harmonize heterogenous databases, and as such specializes in tabular datasets. 

The two major GIS applications ArcGIS and QGIS also have specific \ac{vpl}s attached to their applications. 
The main use-case for these \ac{vpl}s is to automate repetitive workflows within ArcGIS or QGIS. 

Lastly, Geoflow is a much newer \ac{vpl} meant for generic 3D geodata processing.
While this application is still in an early phase,  it already offers very powerful 
range of functions.
It offers CGAL processes like alpha shape, triangulation and line simplification, as well as direct visualization of in-between products.

% In comparison with the three aforementioned \ac{GIS} vpls, geoflow is much richer 

\begin{figure}
\centering
\begin{subfigure}[b]{0.45\linewidth}
  \graphicspath{{../../assets/images/background/geo-vpl/}}
  \centering
  \includegraphics[width=\linewidth]{arcgis.png}
  \caption{}\label{fig:gisvpl:1}
\end{subfigure}%
\qquad 
\begin{subfigure}[b]{0.45\linewidth}
  \graphicspath{{../../assets/images/background/geo-vpl/}}
  \centering
  \includegraphics[width=\linewidth]{fme.png}
  \caption{}\label{fig:gisvpl:2}
\end{subfigure}%
\\
\begin{subfigure}[c]{0.45\linewidth}
  \centering
  \graphicspath{{../../assets/images/background/geo-vpl/}}
  \includegraphics[width=\linewidth]{qgis.png}
  \caption{}\label{fig:gisvpl:3}
\end{subfigure}%
\qquad 
\begin{subfigure}[d]{0.45\linewidth}
  \centering
  \graphicspath{{../../assets/images/background/geo-vpl/}}
  \includegraphics[width=\linewidth]{geoflow.png}
  \caption{}\label{fig:gisvpl:4}
\end{subfigure}%
\caption[GIS VPLs]{Four VPLs used in the field of GIS: ArcGIS's Model Builder (a), Save Software's FME (b), QGIS's Graphical Modeler (c), and Geoflow (d).}
\label{fig:gisvpl}
\end{figure}

\subsection*{ VPLs in neighboring domains }
% - AliceVision Meshroom
% - Rhino: Grasshopper
% - Revit: Dynamo  
% - Blender: Geometry Nodes
% - Houdini: Procedural Modelling
% - Blender: Shader Nodes
% - Adobe: Substance Designer
% - Unreal Engine: Material Nodes
% - Unity: Shader Graph
% - Houdini: Houdini
% - Unreal: Blueprints
% - Unity: Bolt
Besides the use cases already mentioned, a significant number of 
visual programming applications are emerging in fields concerned 
with 2D and 3D geometry creation \& visualization. 

The persistence of visual programming within the field of shaders and
geometry, suggests that visual programming languages are advantageous in
situations where a 'visual' product requires debugging during development. 

These are all popular applications, many users, multiple courses and tutorials, 

For \ac{gis} applications, this area is 

% The field of geo-informatics also appreciates the 'dataflow modelling' aspects (TODO FIX THIS SENTENCE)


% The environment proposed by this thesis builds from Ravi Peter's work. GeoFront can be seen as GeoFlow, but in a web browser. However, the introduction of this web component significantly changes the purpose and use-case of GeoFront compared to GeoFlow. 

% GeoFlow's mission: Create a geoprocessing pipeline in a visual environment, in order to speed up and improve the quality of the development process of this pipeline "FOR YOURSELF", compared to text-based methods. This speed and quality comes from the fact that the visual environment makes rapid experimentation and evaluation possible. This is especially helpful for non-determinant processes, or processes containing 'magic' number parameters. Examples of these are RANSAC algorithms. 

% GeoFront's mission: Use the vpl to make rapid experimentation and evaluation of your geoprocessing functions possible FOR OTHERS. This enables others to rapidly utilize your geoprocessing method. The primary use case of this is collaboration: Rapidly publish ones results, demonstrate reproducibility, retrieve feedback, etc.   


% Visual programming environment for geo-computation (geo-vpl) has been tried before, natively ( Grasshopper , FME , GeoNodes , Blender Geometry Nodes ). 

% We test how well contemporary web technologies support such an application, as well as judge aspects such as accessibility \& performance of said application. We also judge if this type of application is indeed beneficial and usable as a scripting / demo environment.  

% These features could all be implemented by normal means ( buttons, panels, sliders ) -->



\section{Browser-based visual programming}
\label{sec:related-webvpl}

This section is dedicated to visual programming applications running in a browser.
It must be emphasized that of all the various vpls named in \refsec{sec:related-geovpl}, none are browser-based. 
This is likely the case because most of those vpls are computationally intensive, C++-based applications.

Nevertheless, if one looks in other domains 

- name node RED

- name blockly / scratch

The number of visual programming languages written as web applications is surprisingly low. 
Most applications are educational


(https://www.ucode.com/coding-classes-for-kids/is-scratch-the-same-as-blockly)

(https://developers.google.com/blockly/)

(https://developers.googleblog.com/2019/01/scratch-30s-new-programming-blocks.html)

% (SOURCE: https://dl.acm.org/doi/fullHtml/10.1145/1592761.1592779?casa_token=cJ1iX1YYimkAAAAA:YVyp3KFiKwD2GMuBUUIgvibbNsEgndqNQzehRnCosCpyEx51C_uNpi2D4-lsE-x88hQFSWcbTfrP_w)

Does exist, but very niche. 
best example is Scratch, an educational tool.
digging in github reaps some results, mostly experimental, in development applications




JOS: ZODRA JE HIERMEE KLAAR BENT: GA meteen door met de conclusie.

 9:00 -> begin 

16:00 -> done, ga fieten :), naar ouders, en dan aan geon werken! 


\section{Browser-based Visual programming and geocomputation} 

% - Mobius Modeller : https://mobius.design-automation.net/pages/mobius_modeller.html
To the best of the author's knowledge, only one publicly available visual programming language exist which is both able to be configured and executed in a browser, and is able to be used for geodata computation.
This application is called the Möbius modeller (Source: https://mobius-08.design-automation.net/about), and is by far the closest equivalent to the geo-web-vpl proposed by this study.
Though it only uses javascript, the tool is able to be successfully used for an impressive range of applications, including CAD, BIM, urban planning, and GIS. 
It uses a combination of a very 'bare-bones' diagram-based vpl, together with a very rich block-based vpl.
In fact, the block-based vpl is so rich that is almost ceases to be a vpl altogether, and starts to be python-like language with heavy IDE support.  

This study is still different from the mobius modeller in the following aspects: 
\begin{itemize}[-]
  \item This study explores the usage of diagram-based vpls as opposed to a block-based vpl, to allow for the dataflow programming advantages described in \refsec{sec:background:dataflow}.
  \item This study explores the usage of WebAssembly to both improve performance and to use existing geocomputation libraries.
  \item This study addresses the life-cycle issues of \ac{vpl}s stated in \refsec{sec:background:vpl:disadvantages}. 
\end{itemize}

%% SOME THOUGHTS ON RELATED WORKS: 

% GEO: the thing we want to do
% VPL: best choice for end user development, 
% WEB: solves the huge life-cycle problem of publication


% SOME THOUGHTS ABOUT THE IMPORTANCE OF EXPERIMENTATION AND PLAY
% -> testing & reproducability.
% RANSAC -> many 'magic' parameter. 
% Game Of Life -> impossibility of 'proving' behaviour systems. 
% Many parameters simply need to be discovered by 'play' / simulation
% Jonathan blow -> using interactive applications, an intrinsic understanding can be gained without explicit communication.

